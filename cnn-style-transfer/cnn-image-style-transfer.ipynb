{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image style transfer with CNN\n",
    "\n",
    "The goal is to generate new image from two input images the way that the output image contains the content of the first input image (spatial features) and the style (texture) of the another input image. The actual generator is implemented using convolutional neural network (CNN).\n",
    "\n",
    "In other words, define a style transfer process which modifies the content image style while preserving its content close to the original. You can also think it as a process of merging two images together resulting in an output image containing aspects from both input images.\n",
    "\n",
    "This notebook is an implementation of the method used to extract the artistic style of an image described in the following white paper (https://arxiv.org/pdf/1508.06576.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "#### Initialization\n",
    "- Two images (content and style) from which the feature maps are extracted\n",
    "    - Content image, the one containing all the spatial features\n",
    "    - Style image, contains the overall style / texture\n",
    "- The feature maps are used to adjust the features of random generated initial image to be as close as possible to the extracted content and style features\n",
    "- A random generated image is passed as an input to the model\n",
    "- After the style transfer process, the output image is overpainted to the input image by adjusting the pixel values during the process\n",
    "\n",
    "#### Base model\n",
    "- In order to extract the correct feature maps from the input images (spatial features from the content image, style features from the style image), a general machine learning model is needed\n",
    "- There are several models which can be used as a base model to extract the feature maps\n",
    "    - VGG16 is arguably the most popular one, at least for this purpose\n",
    "    - Other models like VGG19, InceptionV3, and ResNet50\n",
    "- The model is only used to get the right features, it will not be trained during the process (in other words, the model weights won't be update)\n",
    "\n",
    "#### The problem definition\n",
    "The style transfer process can be turned into a machine learning optimization problem. The optimization problem here is to minimize the loss function which is defined as follows:\n",
    "\n",
    "Total loss = content loss + style loss\n",
    "\n",
    "Content loss = The difference in content between initial input image and the content image\n",
    "Style loss = The difference in style between the input and the style image\n",
    "\n",
    "The smaller the total loss is, the closer the features of the output image are to the input images => matching content and style of the original images\n",
    "\n",
    "#### The process description\n",
    "- Random initial image\n",
    "- Match the initial image's feature maps to the extracted feature maps at chosen feature convolutional layer => backpropagates the input image pixels instead of the model weights\n",
    "\n",
    "![NST Architecture](https://github.com/thushv89/exercises_thushv_dot_com/raw/dd79478562dd4c0f53472af5b79252404e030838/neural_style_transfer_light_on_math_ml/nst_architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:19.270000Z",
     "start_time": "2019-09-10T19:30:19.266500Z"
    }
   },
   "outputs": [],
   "source": [
    "IMAGE_PARAMS = {\n",
    "    'WIDTH': 512,\n",
    "    'HEIGHT': 512,\n",
    "    'CHANNELS': 3\n",
    "}\n",
    "\n",
    "# The mean value of the training data of the ImageNet in BGR format\n",
    "VGG_MEAN = [123.68, 116.779, 103.939]\n",
    "\n",
    "CONTENT_IMAGE_PATH = \"img/content.png\"\n",
    "STYLE_IMAGE_PATH = \"img/style.png\"\n",
    "\n",
    "CONTENT_WEIGHT = 0.02\n",
    "STYLE_WEIGHT = 4.5\n",
    "\n",
    "# Gatys used layer \"block2_conv2\"\n",
    "# Do experiments with other layers\n",
    "# He only used one layer implying that it produces better results\n",
    "CONTENT_LAYER = \"block2_conv2\"\n",
    "\n",
    "STYLE_LAYERS = [\n",
    "    'block1_conv2',\n",
    "    'block2_conv2',\n",
    "    'block3_conv3',\n",
    "    'block4_conv3',\n",
    "    'block5_conv3'\n",
    "]\n",
    "\n",
    "ITERATIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:19.306500Z",
     "start_time": "2019-09-10T19:30:19.271500Z"
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "def resize_image(image_file_path):\n",
    "    img = Image.open(image_file_path)\n",
    "    img = img.resize((IMAGE_PARAMS['WIDTH'], IMAGE_PARAMS['HEIGHT']))\n",
    "    return img\n",
    "\n",
    "content_image = resize_image(CONTENT_IMAGE_PATH)\n",
    "style_image = resize_image(STYLE_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:19.453000Z",
     "start_time": "2019-09-10T19:30:19.308000Z"
    }
   },
   "outputs": [],
   "source": [
    "display(content_image)\n",
    "display(style_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:19.526500Z",
     "start_time": "2019-09-10T19:30:19.454000Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize the image by the mean values of VGG and transformed to BGR (from RGB)\n",
    "# This is required so the feature maps of VGG are applied correctly\n",
    "def normalize_by(image, bgr_mean):\n",
    "    image = np.asarray(image, dtype=\"float32\")\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    image[:, :, :, 0] -= bgr_mean[2]\n",
    "    image[:, :, :, 1] -= bgr_mean[1]\n",
    "    image[:, :, :, 2] -= bgr_mean[0]\n",
    "\n",
    "    # Change the order to BGR (because of VGG model uses it)\n",
    "    return image[:, :, :, ::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG CNN architecture\n",
    "- Visual Geometry Group (VGG) achieved the object classification error rate of 7% with this model\n",
    "- Effective at object recognition\n",
    "\n",
    "#### Network details\n",
    "- Consists of 16 convolutional layers\n",
    "- ReLU as the activation function\n",
    "- 5 pooling layers\n",
    "- 3 fully connected layers\n",
    "- Architecture images from https://shafeentejani.github.io and http://www.cs.toronto.edu\n",
    "\n",
    "![VGG vertical architecture](https://shafeentejani.github.io/assets/images/style_transfer/vgg_architecture_vertical.png)\n",
    "![VGG architecture](http://www.cs.toronto.edu/~frossard/post/vgg16/vgg16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:20.833000Z",
     "start_time": "2019-09-10T19:30:19.527500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "def build_vgg_input_tensor(content_image, style_image, generated_image):\n",
    "    # Build the VGG16 model\n",
    "    content_image = backend.variable(content_image)\n",
    "    style_image = backend.variable(style_image)\n",
    "\n",
    "    input_tensor = backend.concatenate([\n",
    "        content_image,\n",
    "        style_image,\n",
    "        generated_image\n",
    "    ], axis=0)\n",
    "    \n",
    "    return input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:20.836500Z",
     "start_time": "2019-09-10T19:30:20.834000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze the base model weights and biases, only the generated image pixel values are \"trained\"\n",
    "def freeze_model(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content loss\n",
    "\n",
    "- The lower levels of CNNs are more focused on individual pixel values\n",
    "- The higher levels are composed from the lower levels and thereby forming actual features of the image from the more basic values\n",
    "- Calculate the root mean squared error between the activations of the generated image and the content image\n",
    "- Different feature maps of the higher layers are activated when there's different objects in the images (https://arxiv.org/pdf/1311.2901.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:20.843000Z",
     "start_time": "2019-09-10T19:30:20.839000Z"
    }
   },
   "outputs": [],
   "source": [
    "def content_loss(content_features, generated_features):\n",
    "    return backend.sum(backend.square(generated_features - content_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-08T11:04:09.026403Z",
     "start_time": "2019-09-08T11:04:09.024403Z"
    }
   },
   "source": [
    "## Style loss\n",
    "- Calculated with a set of layers (content used only one)\n",
    "- From the quantifying point of view, style is the amount of correlation between features maps in a layer\n",
    "- Style loss = the difference of correlation between the feature maps of generated image and the style image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:20.855500Z",
     "start_time": "2019-09-10T19:30:20.848000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Also known as style matrix\n",
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(\n",
    "        backend.permute_dimensions(x, (2, 0, 1))\n",
    "    )\n",
    "    return backend.dot(features, backend.transpose(features))\n",
    "\n",
    "def style_loss(style, generated, image_params):\n",
    "    height = style.shape[0]\n",
    "    width = style.shape[1]\n",
    "\n",
    "    gram_style = gram_matrix(style)\n",
    "    gram_generated = gram_matrix(generated)\n",
    "    \n",
    "    channels = image_params['CHANNELS']\n",
    "    height = image_params['HEIGHT']\n",
    "    width = image_params['WIDTH']\n",
    "    # Total style loss of the layer at hand\n",
    "    square_loss = backend.square(gram_generated - gram_style)\n",
    "    \n",
    "    # The hyper parameter from Gatys paper, does not really matter since beta will absorb this one\n",
    "    multiplier = 1. / (4. * (channels ** 2) * ((height * width) ** 2))\n",
    "    return backend.sum(multiplier * square_loss)\n",
    "\n",
    "def total_style_loss(style_layers, layers, image_params):\n",
    "    loss = backend.variable(0.)\n",
    "    for layer_name in style_layers:\n",
    "        layer_features = layers[layer_name]\n",
    "        style_features = layer_features[1, :, :, :]\n",
    "    \n",
    "        combination_features = layer_features[2, :, :, :]\n",
    "    \n",
    "        style_loss_amount = style_loss(style_features, combination_features, image_params)\n",
    "        \n",
    "        # We could assign invidual weights for each layer but keep them equally important for now\n",
    "        loss = loss + (1.0 / len(style_layers)) * style_loss_amount\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:20.861000Z",
     "start_time": "2019-09-10T19:30:20.856500Z"
    }
   },
   "outputs": [],
   "source": [
    "def total_loss(content_layer, style_layers, vgg_model, image_params):\n",
    "    loss = backend.variable(0.)\n",
    "    layers = dict([(layer.name, layer.output) for layer in vgg_model.layers])\n",
    "\n",
    "    # Content loss\n",
    "    content_layer_features = layers[content_layer]\n",
    "    content_features = content_layer_features[0, :, :, :]\n",
    "    generated_features = content_layer_features[2, :, :, :]\n",
    "    loss = loss + CONTENT_WEIGHT * content_loss(content_features, generated_features)\n",
    "  \n",
    "    # Style loss\n",
    "    loss = loss + STYLE_WEIGHT * total_style_loss(style_layers, layers, image_params)\n",
    "    \n",
    "    # TODO\n",
    "    # loss = loss + total_variation_weight * total_variation_loss(combination_image)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:20.869000Z",
     "start_time": "2019-09-10T19:30:20.862000Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_loss_and_gradients(img, generated_image, loss_and_gradients):\n",
    "    img = img.reshape((1, IMAGE_PARAMS['HEIGHT'], IMAGE_PARAMS['WIDTH'], IMAGE_PARAMS['CHANNELS']))\n",
    "    \n",
    "    # Generated image is the current input to the model\n",
    "    # We want to capture the new loss and gradients every round\n",
    "    out = backend.function([generated_image], loss_and_gradients)([img])\n",
    "    loss = out[0]\n",
    "    gradients = out[1].flatten().astype(\"float64\")\n",
    "    return loss, gradients\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, generated_image, loss_and_gradients):\n",
    "        self.generated_image = generated_image\n",
    "        self.loss_and_gradients = loss_and_gradients\n",
    "    \n",
    "    def loss(self, img):\n",
    "        loss, gradients = evaluate_loss_and_gradients(img, self.generated_image, self.loss_and_gradients)\n",
    "        self._gradients = gradients\n",
    "        return loss, gradients\n",
    "\n",
    "    def gradients(self, x):\n",
    "        return self._gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:21.442500Z",
     "start_time": "2019-09-10T19:30:20.870500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:21.450000Z",
     "start_time": "2019-09-10T19:30:21.444000Z"
    }
   },
   "outputs": [],
   "source": [
    "# VGG normalized pixel values back to 0-255 range\n",
    "def restore_image(img, vgg_mean):\n",
    "    img = img.reshape((\n",
    "        IMAGE_PARAMS['HEIGHT'],\n",
    "        IMAGE_PARAMS['WIDTH'],\n",
    "        IMAGE_PARAMS['CHANNELS']\n",
    "    ))\n",
    "\n",
    "    img = img[:, :, ::-1]\n",
    "    \n",
    "    img[:, :, 0] += vgg_mean[2]\n",
    "    img[:, :, 1] += vgg_mean[1]\n",
    "    img[:, :, 2] += vgg_mean[0]\n",
    "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
    "    \n",
    "    return img\n",
    "\n",
    "def save_image(img_array, filename):\n",
    "    image = Image.fromarray(img_array)\n",
    "    image.save(filename)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:30:21.500000Z",
     "start_time": "2019-09-10T19:30:21.451500Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def random_initial_image(image_params):\n",
    "    img_dims = (1, image_params['HEIGHT'], image_params['WIDTH'], image_params['CHANNELS'])\n",
    "    return np.random.uniform(\n",
    "        -128.0,\n",
    "        128.0,\n",
    "        img_dims\n",
    "    )\n",
    "\n",
    "def optimize(img, result_image, loss_and_gradients, iterations, vgg_mean, save_iterations = True):\n",
    "    evaluator = Evaluator(result_image, loss_and_gradients)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        img, loss, info = fmin_l_bfgs_b(\n",
    "            evaluator.loss,             # Function to minimise\n",
    "            img.flatten(),              # Initial guess\n",
    "            #fprime=evaluator.gradients, # The gradient of the evaluator loss function\n",
    "            maxfun=20\n",
    "        )\n",
    "        \n",
    "        print('Iteration %d, current loss %d' % (i, loss))\n",
    "        \n",
    "        if save_iterations:\n",
    "            r = restore_image(img.copy(), vgg_mean)\n",
    "            filename = './process/' + str(i) + '.png'\n",
    "            save_image(r, filename)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T19:30:19.282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Programs\\Anaconda\\envs\\dontcare\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Iteration 0, current loss 84097187840\n",
      "Iteration 1, current loss 36024369152\n",
      "Iteration 2, current loss 20416444416\n",
      "Iteration 3, current loss 16866445312\n",
      "Iteration 4, current loss 15610103808\n",
      "Iteration 5, current loss 14941206528\n",
      "Iteration 6, current loss 14600656896\n",
      "Iteration 7, current loss 14393773056\n",
      "Iteration 8, current loss 14243739648\n",
      "Iteration 9, current loss 14124851200\n",
      "Iteration 10, current loss 14032873472\n",
      "Iteration 11, current loss 13956978688\n",
      "Iteration 12, current loss 13883264000\n",
      "Iteration 13, current loss 13809644544\n",
      "Iteration 14, current loss 13744218112\n",
      "Iteration 15, current loss 13680534528\n",
      "Iteration 16, current loss 13623471104\n",
      "Iteration 17, current loss 13573764096\n"
     ]
    }
   ],
   "source": [
    "# Read and normalize input images\n",
    "content_image = normalize_by(content_image, VGG_MEAN)\n",
    "style_image = normalize_by(style_image, VGG_MEAN)\n",
    "\n",
    "\n",
    "result_image_shape = (\n",
    "    1,\n",
    "    IMAGE_PARAMS['HEIGHT'],\n",
    "    IMAGE_PARAMS['WIDTH'],\n",
    "    IMAGE_PARAMS['CHANNELS']\n",
    ")\n",
    "\n",
    "result_image = backend.placeholder(result_image_shape)\n",
    "\n",
    "# Build the VGG model with the input images + placeholder for the result image'\n",
    "input_tensor = build_vgg_input_tensor(content_image, style_image, result_image)\n",
    "vgg_model = VGG16(input_tensor=input_tensor, include_top=False)\n",
    "\n",
    "# Freeze the model weights and biases, we are only \"training\" the pixel values of the result image\n",
    "vgg_model = freeze_model(vgg_model)\n",
    "\n",
    "initial_loss = total_loss(\n",
    "    CONTENT_LAYER,\n",
    "    STYLE_LAYERS,\n",
    "    vgg_model,\n",
    "    IMAGE_PARAMS\n",
    ")\n",
    "\n",
    "# Gradients return a list of tensors\n",
    "loss_and_gradients = [initial_loss] + backend.gradients(initial_loss, result_image)\n",
    "\n",
    "initial_image = random_initial_image(IMAGE_PARAMS)\n",
    "img = optimize(\n",
    "    initial_image,\n",
    "    result_image,\n",
    "    loss_and_gradients,\n",
    "    ITERATIONS,\n",
    "    VGG_MEAN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-10T19:30:19.284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the final image\n",
    "restored = restore_image(img, VGG_MEAN)\n",
    "save_image(restored, \"./process/final.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T18:01:44.435805Z",
     "start_time": "2019-09-09T18:01:08.427Z"
    }
   },
   "source": [
    "# Further experiments\n",
    "- Change VGG pooling operation from max pooling to avg pooling, might produce better results\n",
    "- Add the total variation loss\n",
    "    - Total variation loss = The smoothness of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "Great and more detailed explanations:\n",
    "- https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
