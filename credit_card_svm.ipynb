{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit-card-svm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnOP2NnLuFa/P9ErThYsCJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laaksonenl/machine-learning/blob/master/credit_card_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7jUwb5doWBl",
        "colab_type": "text"
      },
      "source": [
        "# Credit card fraud detection with SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxCjOM3W_MMA",
        "colab_type": "text"
      },
      "source": [
        "Classify credit card transactions with highly imbalanced dataset (only minor portion of transactions are frauds).\n",
        "\n",
        "The data is anonymized and run through PCA. All V-prefix features are principal components obtained with PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KRwewuJokY9",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "05ce87e8-0da8-4cbe-e6ee-f08b3c79c376"
      },
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "files.upload()\n",
        "\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "! kaggle datasets download -d 'mlg-ulb/creditcardfraud'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-55c67f1d-7638-49d9-a36d-f0113d2ae958\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-55c67f1d-7638-49d9-a36d-f0113d2ae958\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading creditcardfraud.zip to /content\n",
            "100% 66.0M/66.0M [00:00<00:00, 67.7MB/s]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8AYYb-7ozFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d7331987-a5fe-4ee3-9986-fcb72ecc01d2"
      },
      "source": [
        "! unzip /content/creditcardfraud.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/creditcardfraud.zip\n",
            "  inflating: creditcard.csv          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_F-p1-F7G1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/creditcard.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvtVjoazah1L",
        "colab_type": "text"
      },
      "source": [
        "### Data exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97MEkI6w-yyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "dc665f15-7391-4d03-e357-b0c9ec3888a1"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inGwAGqxawUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "0a2707f5-cb3e-4ca7-b14e-37045c8706aa"
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>284807.000000</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>2.848070e+05</td>\n",
              "      <td>284807.000000</td>\n",
              "      <td>284807.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>94813.859575</td>\n",
              "      <td>3.919560e-15</td>\n",
              "      <td>5.688174e-16</td>\n",
              "      <td>-8.769071e-15</td>\n",
              "      <td>2.782312e-15</td>\n",
              "      <td>-1.552563e-15</td>\n",
              "      <td>2.010663e-15</td>\n",
              "      <td>-1.694249e-15</td>\n",
              "      <td>-1.927028e-16</td>\n",
              "      <td>-3.137024e-15</td>\n",
              "      <td>1.768627e-15</td>\n",
              "      <td>9.170318e-16</td>\n",
              "      <td>-1.810658e-15</td>\n",
              "      <td>1.693438e-15</td>\n",
              "      <td>1.479045e-15</td>\n",
              "      <td>3.482336e-15</td>\n",
              "      <td>1.392007e-15</td>\n",
              "      <td>-7.528491e-16</td>\n",
              "      <td>4.328772e-16</td>\n",
              "      <td>9.049732e-16</td>\n",
              "      <td>5.085503e-16</td>\n",
              "      <td>1.537294e-16</td>\n",
              "      <td>7.959909e-16</td>\n",
              "      <td>5.367590e-16</td>\n",
              "      <td>4.458112e-15</td>\n",
              "      <td>1.453003e-15</td>\n",
              "      <td>1.699104e-15</td>\n",
              "      <td>-3.660161e-16</td>\n",
              "      <td>-1.206049e-16</td>\n",
              "      <td>88.349619</td>\n",
              "      <td>0.001727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>47488.145955</td>\n",
              "      <td>1.958696e+00</td>\n",
              "      <td>1.651309e+00</td>\n",
              "      <td>1.516255e+00</td>\n",
              "      <td>1.415869e+00</td>\n",
              "      <td>1.380247e+00</td>\n",
              "      <td>1.332271e+00</td>\n",
              "      <td>1.237094e+00</td>\n",
              "      <td>1.194353e+00</td>\n",
              "      <td>1.098632e+00</td>\n",
              "      <td>1.088850e+00</td>\n",
              "      <td>1.020713e+00</td>\n",
              "      <td>9.992014e-01</td>\n",
              "      <td>9.952742e-01</td>\n",
              "      <td>9.585956e-01</td>\n",
              "      <td>9.153160e-01</td>\n",
              "      <td>8.762529e-01</td>\n",
              "      <td>8.493371e-01</td>\n",
              "      <td>8.381762e-01</td>\n",
              "      <td>8.140405e-01</td>\n",
              "      <td>7.709250e-01</td>\n",
              "      <td>7.345240e-01</td>\n",
              "      <td>7.257016e-01</td>\n",
              "      <td>6.244603e-01</td>\n",
              "      <td>6.056471e-01</td>\n",
              "      <td>5.212781e-01</td>\n",
              "      <td>4.822270e-01</td>\n",
              "      <td>4.036325e-01</td>\n",
              "      <td>3.300833e-01</td>\n",
              "      <td>250.120109</td>\n",
              "      <td>0.041527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.640751e+01</td>\n",
              "      <td>-7.271573e+01</td>\n",
              "      <td>-4.832559e+01</td>\n",
              "      <td>-5.683171e+00</td>\n",
              "      <td>-1.137433e+02</td>\n",
              "      <td>-2.616051e+01</td>\n",
              "      <td>-4.355724e+01</td>\n",
              "      <td>-7.321672e+01</td>\n",
              "      <td>-1.343407e+01</td>\n",
              "      <td>-2.458826e+01</td>\n",
              "      <td>-4.797473e+00</td>\n",
              "      <td>-1.868371e+01</td>\n",
              "      <td>-5.791881e+00</td>\n",
              "      <td>-1.921433e+01</td>\n",
              "      <td>-4.498945e+00</td>\n",
              "      <td>-1.412985e+01</td>\n",
              "      <td>-2.516280e+01</td>\n",
              "      <td>-9.498746e+00</td>\n",
              "      <td>-7.213527e+00</td>\n",
              "      <td>-5.449772e+01</td>\n",
              "      <td>-3.483038e+01</td>\n",
              "      <td>-1.093314e+01</td>\n",
              "      <td>-4.480774e+01</td>\n",
              "      <td>-2.836627e+00</td>\n",
              "      <td>-1.029540e+01</td>\n",
              "      <td>-2.604551e+00</td>\n",
              "      <td>-2.256568e+01</td>\n",
              "      <td>-1.543008e+01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>54201.500000</td>\n",
              "      <td>-9.203734e-01</td>\n",
              "      <td>-5.985499e-01</td>\n",
              "      <td>-8.903648e-01</td>\n",
              "      <td>-8.486401e-01</td>\n",
              "      <td>-6.915971e-01</td>\n",
              "      <td>-7.682956e-01</td>\n",
              "      <td>-5.540759e-01</td>\n",
              "      <td>-2.086297e-01</td>\n",
              "      <td>-6.430976e-01</td>\n",
              "      <td>-5.354257e-01</td>\n",
              "      <td>-7.624942e-01</td>\n",
              "      <td>-4.055715e-01</td>\n",
              "      <td>-6.485393e-01</td>\n",
              "      <td>-4.255740e-01</td>\n",
              "      <td>-5.828843e-01</td>\n",
              "      <td>-4.680368e-01</td>\n",
              "      <td>-4.837483e-01</td>\n",
              "      <td>-4.988498e-01</td>\n",
              "      <td>-4.562989e-01</td>\n",
              "      <td>-2.117214e-01</td>\n",
              "      <td>-2.283949e-01</td>\n",
              "      <td>-5.423504e-01</td>\n",
              "      <td>-1.618463e-01</td>\n",
              "      <td>-3.545861e-01</td>\n",
              "      <td>-3.171451e-01</td>\n",
              "      <td>-3.269839e-01</td>\n",
              "      <td>-7.083953e-02</td>\n",
              "      <td>-5.295979e-02</td>\n",
              "      <td>5.600000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>84692.000000</td>\n",
              "      <td>1.810880e-02</td>\n",
              "      <td>6.548556e-02</td>\n",
              "      <td>1.798463e-01</td>\n",
              "      <td>-1.984653e-02</td>\n",
              "      <td>-5.433583e-02</td>\n",
              "      <td>-2.741871e-01</td>\n",
              "      <td>4.010308e-02</td>\n",
              "      <td>2.235804e-02</td>\n",
              "      <td>-5.142873e-02</td>\n",
              "      <td>-9.291738e-02</td>\n",
              "      <td>-3.275735e-02</td>\n",
              "      <td>1.400326e-01</td>\n",
              "      <td>-1.356806e-02</td>\n",
              "      <td>5.060132e-02</td>\n",
              "      <td>4.807155e-02</td>\n",
              "      <td>6.641332e-02</td>\n",
              "      <td>-6.567575e-02</td>\n",
              "      <td>-3.636312e-03</td>\n",
              "      <td>3.734823e-03</td>\n",
              "      <td>-6.248109e-02</td>\n",
              "      <td>-2.945017e-02</td>\n",
              "      <td>6.781943e-03</td>\n",
              "      <td>-1.119293e-02</td>\n",
              "      <td>4.097606e-02</td>\n",
              "      <td>1.659350e-02</td>\n",
              "      <td>-5.213911e-02</td>\n",
              "      <td>1.342146e-03</td>\n",
              "      <td>1.124383e-02</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>139320.500000</td>\n",
              "      <td>1.315642e+00</td>\n",
              "      <td>8.037239e-01</td>\n",
              "      <td>1.027196e+00</td>\n",
              "      <td>7.433413e-01</td>\n",
              "      <td>6.119264e-01</td>\n",
              "      <td>3.985649e-01</td>\n",
              "      <td>5.704361e-01</td>\n",
              "      <td>3.273459e-01</td>\n",
              "      <td>5.971390e-01</td>\n",
              "      <td>4.539234e-01</td>\n",
              "      <td>7.395934e-01</td>\n",
              "      <td>6.182380e-01</td>\n",
              "      <td>6.625050e-01</td>\n",
              "      <td>4.931498e-01</td>\n",
              "      <td>6.488208e-01</td>\n",
              "      <td>5.232963e-01</td>\n",
              "      <td>3.996750e-01</td>\n",
              "      <td>5.008067e-01</td>\n",
              "      <td>4.589494e-01</td>\n",
              "      <td>1.330408e-01</td>\n",
              "      <td>1.863772e-01</td>\n",
              "      <td>5.285536e-01</td>\n",
              "      <td>1.476421e-01</td>\n",
              "      <td>4.395266e-01</td>\n",
              "      <td>3.507156e-01</td>\n",
              "      <td>2.409522e-01</td>\n",
              "      <td>9.104512e-02</td>\n",
              "      <td>7.827995e-02</td>\n",
              "      <td>77.165000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>172792.000000</td>\n",
              "      <td>2.454930e+00</td>\n",
              "      <td>2.205773e+01</td>\n",
              "      <td>9.382558e+00</td>\n",
              "      <td>1.687534e+01</td>\n",
              "      <td>3.480167e+01</td>\n",
              "      <td>7.330163e+01</td>\n",
              "      <td>1.205895e+02</td>\n",
              "      <td>2.000721e+01</td>\n",
              "      <td>1.559499e+01</td>\n",
              "      <td>2.374514e+01</td>\n",
              "      <td>1.201891e+01</td>\n",
              "      <td>7.848392e+00</td>\n",
              "      <td>7.126883e+00</td>\n",
              "      <td>1.052677e+01</td>\n",
              "      <td>8.877742e+00</td>\n",
              "      <td>1.731511e+01</td>\n",
              "      <td>9.253526e+00</td>\n",
              "      <td>5.041069e+00</td>\n",
              "      <td>5.591971e+00</td>\n",
              "      <td>3.942090e+01</td>\n",
              "      <td>2.720284e+01</td>\n",
              "      <td>1.050309e+01</td>\n",
              "      <td>2.252841e+01</td>\n",
              "      <td>4.584549e+00</td>\n",
              "      <td>7.519589e+00</td>\n",
              "      <td>3.517346e+00</td>\n",
              "      <td>3.161220e+01</td>\n",
              "      <td>3.384781e+01</td>\n",
              "      <td>25691.160000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Time            V1  ...         Amount          Class\n",
              "count  284807.000000  2.848070e+05  ...  284807.000000  284807.000000\n",
              "mean    94813.859575  3.919560e-15  ...      88.349619       0.001727\n",
              "std     47488.145955  1.958696e+00  ...     250.120109       0.041527\n",
              "min         0.000000 -5.640751e+01  ...       0.000000       0.000000\n",
              "25%     54201.500000 -9.203734e-01  ...       5.600000       0.000000\n",
              "50%     84692.000000  1.810880e-02  ...      22.000000       0.000000\n",
              "75%    139320.500000  1.315642e+00  ...      77.165000       0.000000\n",
              "max    172792.000000  2.454930e+00  ...   25691.160000       1.000000\n",
              "\n",
              "[8 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ7VI2Dsa2EQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "f46a35a5-bfdd-4c0f-e5ea-390649fad4eb"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Time      0\n",
              "V1        0\n",
              "V2        0\n",
              "V3        0\n",
              "V4        0\n",
              "V5        0\n",
              "V6        0\n",
              "V7        0\n",
              "V8        0\n",
              "V9        0\n",
              "V10       0\n",
              "V11       0\n",
              "V12       0\n",
              "V13       0\n",
              "V14       0\n",
              "V15       0\n",
              "V16       0\n",
              "V17       0\n",
              "V18       0\n",
              "V19       0\n",
              "V20       0\n",
              "V21       0\n",
              "V22       0\n",
              "V23       0\n",
              "V24       0\n",
              "V25       0\n",
              "V26       0\n",
              "V27       0\n",
              "V28       0\n",
              "Amount    0\n",
              "Class     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAYjX5YiAQar",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e1fcd09-0db8-486e-d79b-bcaafc0ed5ed"
      },
      "source": [
        "def calculate_positive_portion(data):\n",
        "  fraud_pct =  data.value_counts()[1] / data.shape[0]\n",
        "  print('Fraud portion', fraud_pct, '%')\n",
        "\n",
        "calculate_positive_portion(data['Class'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fraud portion 0.001727485630620034 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfAv78XKt9Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.drop('Class', axis=1)\n",
        "y = data['Class']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY1ph6jMbPa5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "outputId": "e19e37c1-24dd-4a78-d588-04e6fdaeb19b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "amount = data['Amount']\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.title('Transaction amount')\n",
        "plt.xlim((0, amount.max()))\n",
        "\n",
        "sns.distplot(amount.values, color='g')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c647e6518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxyR4XfwbP3x",
        "colab_type": "text"
      },
      "source": [
        "The data description told us, that PCA was used before. In order to use PCA, the dataset must be scaled, so we can skip standardization part in data preparation phase. Only time and amount columns are not scaled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ojpkxxtygug",
        "colab_type": "text"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "With SVM models, it's important to scale the features, otherwise SVM will neglect the smaller feature scales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNqbxf6umvX_",
        "colab_type": "text"
      },
      "source": [
        "### Scaling\n",
        "\n",
        "Scale amount and time columns with RobustScaler. RobustScaler focuses on the part where the most data is. Because RobustScaler uses interquartile range instead of min and max values, it's robust to outliers than typical normalization. StandardScaler is another option, but it does not work well if the data does not follow Gaussian distribution already."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNfRNrGTnYU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "rs = RobustScaler()\n",
        "\n",
        "data['scaled_time'] = rs.fit_transform(data['Time'].values.reshape(-1, 1))\n",
        "data['scaled_amount'] = rs.fit_transform(data['Amount'].values.reshape(-1, 1))\n",
        "\n",
        "data.drop(['Time', 'Amount'], axis=1, inplace=True)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DsYP6TDoM9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaled_time = data['scaled_time']\n",
        "scaled_amount = data['scaled_amount']\n",
        "\n",
        "data.drop(['scaled_time', 'scaled_amount'], axis=1, inplace=True)\n",
        "data.insert(0, 'scaled_time', scaled_time)\n",
        "data.insert(1, 'scaled_amount', scaled_amount)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD0EDP6LqZFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "2f201663-91b1-4fe8-baf7-c081fa8cf32d"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>scaled_time</th>\n",
              "      <th>scaled_amount</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.994983</td>\n",
              "      <td>1.783274</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.994983</td>\n",
              "      <td>-0.269825</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.994972</td>\n",
              "      <td>4.983721</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.994972</td>\n",
              "      <td>1.418291</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.994960</td>\n",
              "      <td>0.670579</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   scaled_time  scaled_amount        V1  ...       V27       V28  Class\n",
              "0    -0.994983       1.783274 -1.359807  ...  0.133558 -0.021053      0\n",
              "1    -0.994983      -0.269825  1.191857  ... -0.008983  0.014724      0\n",
              "2    -0.994972       4.983721 -1.358354  ... -0.055353 -0.059752      0\n",
              "3    -0.994972       1.418291 -0.966272  ...  0.062723  0.061458      0\n",
              "4    -0.994960       0.670579 -1.158233  ...  0.219422  0.215153      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qxKAev1lNMU",
        "colab_type": "text"
      },
      "source": [
        "#### Rebalancing the data\n",
        "\n",
        "We need to deal with heavily imbalanced dataset (almost non-existing portion of frauds) so that our model can detect the patterns between fraudulent and non-fraudulent.\n",
        "\n",
        "Keypoints:\n",
        "- Train the model with balanced data so the model will detect the patterns\n",
        "- Test the model with original data to get unbiased results about the performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwcW_vD4t6vK",
        "colab_type": "text"
      },
      "source": [
        "#### Undersampling\n",
        "Remove some data to get balanced (50/50) set of fraudulent and non-fraudulent samples.\n",
        "\n",
        "We have 492 fraudulent samples which means we reduce non-fraudulent samples down to 492. Notice, that this will cause a massive information loss on the part of non-fraudulent patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsB24m9WvZpt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle data by resampling\n",
        "data = data.sample(frac=1)\n",
        "fraudulent = data[data['Class'] == 1]\n",
        "non_fraudulent = data[data['Class'] == 0][:492]\n",
        "\n",
        "# Shuffle balanced data again so model does not pick any correlation from the order\n",
        "balanced_df = pd.concat([fraudulent, non_fraudulent]).sample(frac=1, random_state=42)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xh13gg_IwIV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "33020e9c-7aa6-4827-8e88-1c2586307e54"
      },
      "source": [
        "sns.countplot('Class', data=balanced_df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c587e4240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSF6I9DgwVFZ",
        "colab_type": "text"
      },
      "source": [
        "#### Correlations\n",
        "\n",
        "Correlation analysis should be done on the subsample to detect highly correlated features.\n",
        "\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfMQcmwBxAuz",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### Outlier removal\n",
        "TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T0-viXCRRd7",
        "colab_type": "text"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY6T9qnAzXl8",
        "colab_type": "text"
      },
      "source": [
        "We'll use SVC as our main classifier with undersampled data.\n",
        "The data should be undersampled DURING THE CROSSVALIDATION, otherwise the model most probably overfits and gives us biased results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVOpfEvBRf0O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "b5222cbb-900e-4241-fc4e-4f5ee3e96585"
      },
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "clf = LinearSVC()\n",
        "\n",
        "# Accuracy, no oversampling\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=45)\n",
        "scores = cross_val_score(clf, X, y, cv=5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjN89TDK_Hep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f19ba3fd-352f-4b92-f273-53cec5b585cc"
      },
      "source": [
        "print(\"Training score \", round(scores.mean(), 2) * 100, \"% accuracy score\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training score  100.0 % accuracy score\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTYspD0d_XMF",
        "colab_type": "text"
      },
      "source": [
        "Accuracy for classification is 100%. This means nothing since the data is heavily balanced. Other metrics like precision/recall and ROC should be used as evaluation metric for classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkPdglnJH20E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "\n",
        "def plot_learning_curve(clf, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    fig = plt.figure(figsize=(20,14))\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    \n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        clf, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    \n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    ax.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"#ff9124\")\n",
        "    fig.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
        "    fig.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n",
        "             label=\"Training score\")\n",
        "    fig.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n",
        "             label=\"Cross-validation score\")\n",
        "    fig.ax.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n",
        "    fig.set_xlabel('Training size (m)')\n",
        "    fig.set_ylabel('Score')\n",
        "    fig.grid(True)\n",
        "    fig.legend(loc=\"best\")\n",
        "    \n",
        "    return plt"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsu_Kd4IhG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
        "plot_learning_curve(clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvcOhZfBHDLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def display_scores(scores):\n",
        "#     print(\"Scores:\", scores)\n",
        "#     print(\"Mean:\", scores.mean())\n",
        "#     print(\"Standard deviation:\", scores.std())\n",
        "\n",
        "# display_scores(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao-iOIlCytlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.svm import LinearSVC\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# pipeline = make_pipeline(StandardScaler(), LinearSVC(C=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPU91lpp7cbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import StratifiedKFold\n",
        "# skf = StratifiedKFold(n_splits=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLVTJwEdGTHf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import cross_val_score\n",
        "# scores = cross_val_score(pipeline, X, y, cv=skf, scoring='recall')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9PZ3goi1BMm",
        "colab_type": "text"
      },
      "source": [
        "### Model evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI0FvlA0Qy-A",
        "colab_type": "text"
      },
      "source": [
        "The focus is to find maximize recall particularly which means that we want our model to detect as many positive cases as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lirjc77ewlUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, classification_report, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_precision_recall_curve(y_true, y_pred):\n",
        "  precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "  plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
        "  plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.ylim([0.0, 1.1])\n",
        "  plt.xlim([0.0, 1.0])\n",
        "\n",
        "def report_results(clf, X, y):\n",
        "  print('Number of normal transactions:', len(y[y == 0]))\n",
        "  print('Number of frauds:', len(y[y == 1]))\n",
        "  \n",
        "  clf_report = classification_report(y_true, y_pred)\n",
        "  print(clf_report)\n",
        "\n",
        "  plot_precision_recall_curve(y_true, y_pred)\n",
        "  plot_confusion_matrix(clf, X, y)\n",
        "  plot_roc_curve(clf, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}