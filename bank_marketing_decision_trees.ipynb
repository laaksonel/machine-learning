{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bank-marketing-decision-trees.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "cFq6VUKhhaFd"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPORv1ms++6MyfJLfLT68df",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laaksonenl/machine-learning/blob/master/bank_marketing_decision_trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRtl_kso8XUb"
      },
      "source": [
        "# Bank markerting classification with decision trees\n",
        "\n",
        "**Goal:**\n",
        "\n",
        "The main goal is to develop a predictive model which will tell if a client is open to create a term deposit based on the client's characteristics\n",
        "\n",
        "**Hypothesis:**\n",
        "\n",
        "The base hypothesis is that the wealthier, middle-aged people are the most potential customers in regards of the deposit.\n",
        "\n",
        "**Data Set Information:**\n",
        "\n",
        "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.\n",
        "\n",
        "There are four datasets:\n",
        "1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n",
        "2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\n",
        "3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs).\n",
        "4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs).\n",
        "The smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM).\n",
        "\n",
        "The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KRwewuJokY9",
        "outputId": "4c581c49-67d2-406b-a893-24e84525a61f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip \n",
        "! unzip /content/bank.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-01 17:18:27--  https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 579043 (565K) [application/x-httpd-php]\n",
            "Saving to: ‘bank.zip.2’\n",
            "\n",
            "bank.zip.2          100%[===================>] 565.47K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-01 17:18:27 (3.89 MB/s) - ‘bank.zip.2’ saved [579043/579043]\n",
            "\n",
            "Archive:  /content/bank.zip\n",
            "replace bank-full.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbIIF0Yd73-K"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/bank-full.csv', sep=';')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8VD_wANLQWc"
      },
      "source": [
        "print(df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB-5r3TzLj51"
      },
      "source": [
        "## Exploratory analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWWr9OdrLtpn"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8HlN2lR7B8l"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nniscRWFOLte"
      },
      "source": [
        "### Numerical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0kqFexOLnM8"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtZZjcAXOREH"
      },
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_numericals(data, features):\n",
        "  fig, axes = plt.subplots(2,4, figsize=(25,10))\n",
        "  for ax, f in zip(axes.ravel(), features):\n",
        "    sns.histplot(data[f], ax=ax, kde=True)\n",
        "\n",
        "numericals = df.select_dtypes([np.number]).columns\n",
        "plot_numericals(df, numericals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5Mt8E_AOmsL"
      },
      "source": [
        "#### Age\n",
        "Main focus has been on people with age between 30 and 40. Young people might not have that much extra cash and old people might not want to take term deposit, at least not for years forward.\n",
        "\n",
        "#### Account balance (eur)\n",
        "Most of the people have balance between 0 and 10 000 euros\n",
        "\n",
        "#### Last contact day of the month\n",
        "Does not tell much, this is column should be analyzed with month feature which at this point is in categorical format (string values).\n",
        "Days with less contacts might fall in weekends.\n",
        "\n",
        "#### Phone call duration\n",
        "On average calls took 4-5 minutes. There's much longer calls though. The longest reported call has taken 3000 seconds ( = 50 minutes).\n",
        "Maybe there's correlation with phone duration and term deposits since bank needs to ask all kind of information for term deposit. Of course this depends on the bank's own practices.\n",
        "\n",
        "#### Contacts during the campaign\n",
        "Mostly only a few contacts per customer, the data does not tell us if all the contacts are about term deposit. It's hard to imagine situation where bank contacts over 10 times the same client about the same term deposit unless there's lots of confusion between these parties.\n",
        "\n",
        "#### Number of days passed after the last contact of the previous campaign\n",
        "Most of the clients were not part of the last campaign thus the peak at the start.\n",
        "\n",
        "#### Number of contacts before this campaign\n",
        "No previous or only a few were conducted for the most of the clients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yna-aq1Dw-b-"
      },
      "source": [
        "original_df = df.copy()\n",
        "\n",
        "def label_to_numerical(df):\n",
        "  return df['y'].map({ 'yes': 1, 'no': 0}).astype('uint8')\n",
        "\n",
        "# Convert to numeric so that target will be shown in correlation matrix\n",
        "df['y'] = label_to_numerical(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IsHilPXFQbe"
      },
      "source": [
        "corr = df.corr()\n",
        "sns.heatmap(corr, annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9_xKHEnGRXZ"
      },
      "source": [
        "Only call duration seems to have decent correlation with target label.\n",
        "Notice that Pearson's correlation captures only linear correlations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl9G9XfwIpHE"
      },
      "source": [
        "### Categorical features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnoorBsFIrcO"
      },
      "source": [
        "categoricals = df.select_dtypes(['object'])\n",
        "categoricals.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8s2JX9MMQ4u"
      },
      "source": [
        "def plot_categorical_count(ax, data, feature):\n",
        "  value_counts = data[feature].value_counts()\n",
        "  plot = sns.countplot(data=data, x=feature, ax=ax, order = value_counts.index)\n",
        "\n",
        "  plot.set_xticklabels(\n",
        "    plot.get_xticklabels(), \n",
        "    rotation=45, \n",
        "    horizontalalignment='right',\n",
        "    fontweight='light',\n",
        "    fontsize='x-large'\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6601ENKXvl"
      },
      "source": [
        "count = len(categoricals.columns)\n",
        "fig, axes = plt.subplots(int(count / 4), 4, figsize=(20,10))\n",
        "fig.tight_layout(w_pad=5.0, h_pad=10.0)\n",
        "\n",
        "for ax, c in zip(axes.ravel(), categoricals):\n",
        "  plot_categorical_count(ax, df, c)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ND8WQTec91"
      },
      "source": [
        "- The campaign seems to focus on people with medium income, higher degree and without previous loans or defaults\n",
        "- Interestingly enough, majority of the clients have no loan but own a house which means that the focus is on the wealthy people"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRWqHiPrhgVy"
      },
      "source": [
        "def plot_label_count(ax, data, feature):\n",
        "  ax.tick_params(axis='x', labelrotation=45)\n",
        "  sns.countplot(x=feature, hue='y', data=data, ax=ax, palette=['#FF0000',\"#00FF00\"])\n",
        "\n",
        "count = len(categoricals.columns)\n",
        "fig, axes = plt.subplots(int(count / 4), 4, figsize=(20,10))\n",
        "fig.tight_layout(w_pad=5.0, h_pad=10.0)\n",
        "\n",
        "for ax, c in zip(axes.ravel(), categoricals):\n",
        "  plot_label_count(ax, df, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-CXxjqJ34Yl"
      },
      "source": [
        "### Data preparation and cleaning\n",
        "\n",
        "- No missing data so we don't have to impute any values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z2QgCxXzbgQ"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPLwHUyWsjWC"
      },
      "source": [
        "df['poutcome'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GueCsV1i3760"
      },
      "source": [
        "#### Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeQbKPYC3_oj"
      },
      "source": [
        "# def calculate_outlier_bound(data):\n",
        "#   q1, q3 = np.percentile(data,[25,75])\n",
        "#   iqr = q3 - q1\n",
        "#   upper_bound = q3 +(1.5 * iqr) \n",
        "#   return upper_bound\n",
        "\n",
        "# fig, axes = plt.subplots(1,3, figsize=(20,10))\n",
        "# features = ['age', 'balance', 'duration']\n",
        "# for ax, f in zip(axes, features):\n",
        "#   boxplot = sns.boxplot(y=f, data=numlabel, ax=ax)\n",
        "#   print(boxplot.get_xticks())\n",
        "#   upper_bound = calculate_outlier_bound(df[f])\n",
        "#   boxplot.text(x=0, y=upper_bound, s=str(upper_bound), color='#660000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83WZMqPVu7VF"
      },
      "source": [
        "# def remove_outliers(data, features):\n",
        "#   tmp = data.copy()\n",
        "#   for f in features:\n",
        "#     upper_limit = calculate_outlier_bound(data[f])\n",
        "#     outlier_idxs = tmp[tmp[f] > upper_limit].index\n",
        "#     tmp.drop(outlier_idxs, axis=0, inplace=True)\n",
        "#   return tmp\n",
        "\n",
        "#clean = remove_outliers(df, features)\n",
        "#clean = df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAbI4RaX-S7l"
      },
      "source": [
        "# Group durations to closest full minute\n",
        "#clean['duration'] = np.round(clean['duration'] / 60).astype('uint8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "floUQuS_Gsp_"
      },
      "source": [
        "### Building a pipeline for the data transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CqZxhFzGw-E"
      },
      "source": [
        "# features = clean.drop('y', axis=1)\n",
        "# labels = clean['y']\n",
        "\n",
        "# clean[categoricals.columns].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj4VFsee7--N"
      },
      "source": [
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import make_column_selector\n",
        "\n",
        "def binarize(df):\n",
        "  return df.map({'yes': 1, 'no': 0}) \n",
        "\n",
        "binarizer = FunctionTransformer(binarize)\n",
        "\n",
        "# numerical_columns = df.select_dtypes(exclude='object').columns\n",
        "\n",
        "categorical_transform = make_column_transformer(\n",
        "        # ('passthrough', make_column_selector(dtype_include=np.number)),\n",
        "        (binarizer, ['housing', 'loan', 'default']),\n",
        "        (OneHotEncoder(), ['job', 'month', 'marital', 'education', 'contact']),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7xUNgJjuxr6"
      },
      "source": [
        "num_binarizer = FunctionTransformer(lambda x: int(x > 0))\n",
        "duration_minutes = FunctionTransformer(lambda x: np.ceil(x / 60))\n",
        "\n",
        "numerical_transform = make_column_transformer(\n",
        "    (num_binarizer, ['previous']),\n",
        "    (duration_minutes, ['duration'])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyFz4Jzl8jgt"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlMIOBxeyEH1",
        "collapsed": true
      },
      "source": [
        "from sklearn.pipeline import make_union\n",
        "\n",
        "full_pipeline = make_union(\n",
        "  numerical_transform,\n",
        "  categorical_transform\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHNoCASc0yZh"
      },
      "source": [
        "### Splitting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_hsba0c8obA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('y', axis = 1)\n",
        "y = df['y']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEtQk8DeyVyH"
      },
      "source": [
        "full_pipeline.fit_transform(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-JLyFTeH5NG"
      },
      "source": [
        "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import make_pipeline, Pipeline\n",
        "\n",
        "\n",
        "# # Sklearn knows nothing about Pandas DataFrame, it accepts only numpy arrays\n",
        "# num_attribs = list(numericals)\n",
        "# cat_attribs = list(categoricals.drop('y', axis=1).columns)\n",
        "\n",
        "# # FeatureUnion applies different transformers to the WHOLE of the input data\n",
        "# # ColumnTransformer applies different transformers to different SUBSETS of the whole input data\n",
        "# full_pipeline = ColumnTransformer([\n",
        "#         (\"numerical\", num_pipeline, num_attribs),\n",
        "#         (\"categorical\", cat_pipeline, cat_attribs)\n",
        "#     ])\n",
        "\n",
        "# le = LabelEncoder()\n",
        "# prepared_labels = le.fit_transform(labels)\n",
        "\n",
        "# prepared_features = categorical_transform.fit_transform(features, prepared_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PceD5gofO9Kf"
      },
      "source": [
        "# from sklearn.base import BaseEstimator, TransformerMixin\n",
        "# class TargetEncoder(BaseEstimator, TransformerMixin):\n",
        "#     \"\"\"Target encoder.\n",
        "    \n",
        "#     Replaces categorical column(s) with the mean target value for\n",
        "#     each category.\n",
        "\n",
        "#     \"\"\"\n",
        "    \n",
        "#     def __init__(self, cols=None):\n",
        "#         \"\"\"Target encoder\n",
        "        \n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         cols : list of str\n",
        "#             Columns to target encode.  Default is to target \n",
        "#             encode all categorical columns in the DataFrame.\n",
        "#         \"\"\"\n",
        "#         if isinstance(cols, str):\n",
        "#             self.cols = [cols]\n",
        "#         else:\n",
        "#             self.cols = cols\n",
        "        \n",
        "        \n",
        "#     def fit(self, X, y):\n",
        "#         \"\"\"Fit target encoder to X and y\n",
        "        \n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "#             DataFrame containing columns to encode\n",
        "#         y : pandas Series, shape = [n_samples]\n",
        "#             Target values.\n",
        "            \n",
        "#         Returns\n",
        "#         -------\n",
        "#         self : encoder\n",
        "#             Returns self.\n",
        "#         \"\"\"\n",
        "        \n",
        "#         # Encode all categorical cols by default\n",
        "#         if self.cols is None:\n",
        "#             self.cols = [col for col in X \n",
        "#                          if str(X[col].dtype)=='object']\n",
        "\n",
        "#         # Check columns are in X\n",
        "#         for col in self.cols:\n",
        "#             if col not in X:\n",
        "#                 raise ValueError('Column \\''+col+'\\' not in X')\n",
        "\n",
        "#         # Encode each element of each column\n",
        "#         self.maps = dict() #dict to store map for each column\n",
        "#         for col in self.cols:\n",
        "#             tmap = dict()\n",
        "#             uniques = X[col].unique()\n",
        "#             for unique in uniques:\n",
        "#                 tmap[unique] = y[X[col]==unique].mean()\n",
        "#             self.maps[col] = tmap\n",
        "            \n",
        "#         return self\n",
        "\n",
        "        \n",
        "#     def transform(self, X, y=None):\n",
        "#         \"\"\"Perform the target encoding transformation.\n",
        "        \n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "#             DataFrame containing columns to encode\n",
        "            \n",
        "#         Returns\n",
        "#         -------\n",
        "#         pandas DataFrame\n",
        "#             Input DataFrame with transformed columns\n",
        "#         \"\"\"\n",
        "#         Xo = X.copy()\n",
        "#         for col, tmap in self.maps.items():\n",
        "#             vals = np.full(X.shape[0], np.nan)\n",
        "#             for val, mean_target in tmap.items():\n",
        "#                 vals[X[col]==val] = mean_target\n",
        "#             Xo[col] = vals\n",
        "#         return Xo\n",
        "            \n",
        "            \n",
        "#     def fit_transform(self, X, y=None):\n",
        "#         \"\"\"Fit and transform the data via target encoding.\n",
        "        \n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         X : pandas DataFrame, shape [n_samples, n_columns]\n",
        "#             DataFrame containing columns to encode\n",
        "#         y : pandas Series, shape = [n_samples]\n",
        "#             Target values (required!).\n",
        "\n",
        "#         Returns\n",
        "#         -------\n",
        "#         pandas DataFrame\n",
        "#             Input DataFrame with transformed columns\n",
        "#         \"\"\"\n",
        "#         return self.fit(X, y).transform(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjv2MX2Z9LKJ"
      },
      "source": [
        "pd.DataFrame(prepared_features).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdVcCogofa7G"
      },
      "source": [
        "### Training a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Uyv7PhfdCB"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(prepared_features, prepared_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plWm4f5gxjmh"
      },
      "source": [
        "def plot_precision_recall_curve(y_true, y_pred):\n",
        "  precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
        "  plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
        "  plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.ylim([0.0, 1.1])\n",
        "  plt.xlim([0.0, 1.0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wljBFe0YwupR"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, plot_roc_curve, precision_recall_curve, plot_confusion_matrix\n",
        "\n",
        "def report_results(clf, X, y, preds):\n",
        "  print('Number of non-subscribed:', len(y[y == 0]))\n",
        "  print('Number of subscribed:', len(y[y == 1]))\n",
        "  \n",
        "  clf_report = classification_report(y, preds)\n",
        "  print(clf_report)\n",
        "\n",
        "  plot_precision_recall_curve(y, preds)\n",
        "  plot_confusion_matrix(clf, X, y)\n",
        "  plot_roc_curve(clf, X, y)\n",
        "  auc = roc_auc_score(y, clf.predict_proba(X)[:,1])\n",
        "  print('AUC score: {}'.format(auc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8VjRW30h75s"
      },
      "source": [
        "predictions = dtc.predict(prepared_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLeQXMWixIQ0"
      },
      "source": [
        "report_results(dtc, prepared_features, prepared_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H86YReiDzaMx"
      },
      "source": [
        "Seems like the model overfit the data hard.\n",
        "Let's try with unseen data since we have another, full dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yl8zb65LzhH"
      },
      "source": [
        "#### Handling overfitting\n",
        "\n",
        "Techniques:\n",
        "- Regularization\n",
        "- More meaningful (latent) features, getting rid of the higher degree features\n",
        "- More data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iyht2MKxtIU7"
      },
      "source": [
        "# Use unseen validation set\n",
        "prepared_validation_features = full_pipeline.transform(validation_df.drop('y', axis=1))\n",
        "prepared_validation_labels = le.transform(validation_df['y'])\n",
        "predictions = dtc.predict(prepared_validation_features)\n",
        "\n",
        "report_results(dtc, prepared_validation_features, prepared_validation_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GzW_EDX1EHM"
      },
      "source": [
        "# Create more balanced dataset for training\n",
        "df_full['y'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2JCFMb1Lxf"
      },
      "source": [
        "no = df_full[df_full['y'] == 'no'][:4000]\n",
        "yes = df_full[df_full['y'] == 'yes'][:4000]\n",
        "\n",
        "balanced_df = pd.concat([no, yes])\n",
        "\n",
        "# Shuffle dataset so model won't pick up any correlations from the order\n",
        "balanced_df = balanced_df.sample(frac=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOky-hIy1hIU"
      },
      "source": [
        "balanced_features = balanced_df.drop('y', axis=1)\n",
        "features = full_pipeline.transform(balanced_features)\n",
        "labels = le.transform(balanced_df['y'])\n",
        "\n",
        "dtc2 = DecisionTreeClassifier(max_depth=8)\n",
        "dtc2.fit(features, labels)\n",
        "\n",
        "predictions = dtc2.predict(prepared_validation_features)\n",
        "report_results(dtc2, prepared_validation_features, prepared_validation_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX2ftmTE3A7H"
      },
      "source": [
        "for name, importance in zip(balanced_features.columns, dtc2.feature_importances_):\n",
        "    print(name, importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYKkC6xSIK_Q"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "\n",
        "cv = StratifiedKFold(shuffle=True, n_splits=5, random_state=42)\n",
        "\n",
        "grid_params = [{\n",
        "    'max_depth': [1, 3, 5, 10, 20, 30],\n",
        "    'min_samples_leaf': [1, 10, 50, 100]\n",
        "}]\n",
        "\n",
        "dtc3 = DecisionTreeClassifier()\n",
        "grid_search = GridSearchCV(dtc3, grid_params, cv=cv,\n",
        "                           scoring='recall',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(features, labels)\n",
        "\n",
        "print('Best params', grid_search.best_params_)\n",
        "print('Best estimator', grid_search.best_estimator_)\n",
        "print('Mean test score', grid_search.cv_results_['mean_test_score'])\n",
        "print('Test score deviation', grid_search.cv_results_['std_test_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0hdSae4Z3LR"
      },
      "source": [
        "best_model = grid_search.best_estimator_\n",
        "predictions = best_model.predict(prepared_validation_features)\n",
        "report_results(best_model, prepared_validation_features, prepared_validation_labels, predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3jGS5GguPcq"
      },
      "source": [
        "### Testing ensemble method (RandomForestClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1uhz1ept-Jf"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42, oob_score=True)\n",
        "\n",
        "grid_params_rf = [{\n",
        "                'criterion': ['entropy'],\n",
        "                'min_samples_leaf': [80, 100],\n",
        "                'max_depth': [25, 27],\n",
        "                'min_samples_split': [3, 5],\n",
        "                'n_estimators' : [60, 70]\n",
        "              }]\n",
        "\n",
        "gridsearch_rf = GridSearchCV(rf, param_grid=grid_params_rf, scoring='accuracy', cv=cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81y4f_U5uuSy"
      },
      "source": [
        "gridsearch_rf.fit(features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jrxvu_ifu7TF"
      },
      "source": [
        "print('Best params', gridsearch_rf.best_params_)\n",
        "print('Best estimator', gridsearch_rf.best_estimator_)\n",
        "print('Mean test score', gridsearch_rf.cv_results_['mean_test_score'])\n",
        "print('Test score deviation', gridsearch_rf.cv_results_['std_test_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8IFUbdTxDCT"
      },
      "source": [
        "predictions = gridsearch_rf.best_estimator_.predict(prepared_validation_features)\n",
        "report_results(gridsearch_rf.best_estimator_, prepared_validation_features, prepared_validation_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1G_qEF9vvla"
      },
      "source": [
        "obb = []\n",
        "estimator_count = list(range(5, 150, 5))\n",
        "\n",
        "for i in estimator_count:\n",
        "    random_forest = RandomForestClassifier(n_estimators=i, criterion='entropy',\n",
        "                                           random_state=42, oob_score=True,\n",
        "                                           max_depth=25, min_samples_leaf=80,\n",
        "                                           min_samples_split=3)\n",
        "    random_forest.fit(features, labels)\n",
        "    obb.append(random_forest.oob_score_)\n",
        "\n",
        "max_obb = max(obb)\n",
        "optimal_estimator_count = estimator_count[np.argmax(obb)]\n",
        "print('Max oob {} at number of estimators {}'.format(max_obb, optimal_estimator_count))\n",
        "plt.plot(estimator_count, obb)\n",
        "plt.xlabel('Estimator count')\n",
        "plt.ylabel('Out-of-bag (oob) score')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UulrLacd63Dn"
      },
      "source": [
        "rf = RandomForestClassifier(n_estimators=optimal_estimator_count, criterion='entropy',\n",
        "                                        random_state=42, oob_score=True,\n",
        "                                        max_depth=25, min_samples_leaf=80,\n",
        "                                        min_samples_split=3)\n",
        "\n",
        "y_prepared = le.fit_transform(df_full['y'])\n",
        "X_prepared = full_pipeline.fit_transform(df_full.drop('y', axis=1), y_prepared)\n",
        "rf.fit(X_prepared, y_prepared)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv_GZgRs7VlI"
      },
      "source": [
        "predictions = rf.predict(prepared_validation_features)\n",
        "report_results(rf, prepared_validation_features, prepared_validation_labels, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFq6VUKhhaFd"
      },
      "source": [
        "### Final evaluation on test set\n"
      ]
    }
  ]
}